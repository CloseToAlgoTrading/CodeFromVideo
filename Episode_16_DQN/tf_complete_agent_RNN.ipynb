{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tf-agent",
   "display_name": "tf-agent"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('Enviroment')\n",
    "from tf_trade_enviroment import MyTradeEnv\n",
    "\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import q_rnn_network\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from learningClass import learningHelper\n",
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(12)\n",
    "tf.print(tf.config.list_physical_devices('GPU') )\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "source": [
    "## Read OCHLV data from the file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         High         Low        Open       Close      Volume\n",
       "0  111.440002  107.349998  111.389999  109.330002  53204600.0\n",
       "1  108.650002  105.410004  108.290001  106.250000  64285500.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Open</th>\n      <th>Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>111.440002</td>\n      <td>107.349998</td>\n      <td>111.389999</td>\n      <td>109.330002</td>\n      <td>53204600.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>108.650002</td>\n      <td>105.410004</td>\n      <td>108.290001</td>\n      <td>106.250000</td>\n      <td>64285500.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df = df.drop(['Date','Adj Close'], axis=1)\n",
    "train_index = 200\n",
    "df_train = df.iloc[:train_index]\n",
    "df_test = df.iloc[train_index:]\n",
    "df_train.head(2)"
   ]
  },
  {
   "source": [
    "## Create and validate enviroment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3)\ntime_step_spec.observation: {'price': BoundedArraySpec(shape=(20, 5), dtype=dtype('float32'), name='obs_price', minimum=0.0, maximum=3.4028234663852886e+38), 'pos': BoundedArraySpec(shape=(2,), dtype=dtype('int32'), name='obs_pos', minimum=0, maximum=1)}\ntime_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\ntime_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\ntime_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "environment = MyTradeEnv(df_test)\n",
    "utils.validate_py_environment(environment, episodes=2)\n",
    "\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', environment.time_step_spec().reward)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Create traning and validation enviroment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(MyTradeEnv(df_train))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(MyTradeEnv(df_test))"
   ]
  },
  {
   "source": [
    "## Define trainig paramters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3  "
   ]
  },
  {
   "source": [
    "## Create q_network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network configuration\n",
    "input_fc_layer_params = (40,)\n",
    "lstm_size=(20,)\n",
    "output_fc_layer_params=(20,)\n",
    "\n",
    "# as we are using dictionary in our enviroment, we will create preprocessing layer\n",
    "preprocessing_layers = {\n",
    "    'price': tf.keras.layers.Flatten(),\n",
    "    'pos': tf.keras.layers.Dense(2)\n",
    "    }\n",
    "preprocessing_combiner = tf.keras.layers.Concatenate(axis=-1)\n",
    "\n",
    "#create a q_RNNnet\n",
    "q_net = q_rnn_network.QRnnNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layers,\n",
    "    preprocessing_combiner=preprocessing_combiner,\n",
    "    input_fc_layer_params=input_fc_layer_params,\n",
    "    lstm_size=lstm_size,\n",
    "    output_fc_layer_params=output_fc_layer_params)    "
   ]
  },
  {
   "source": [
    "## Create the DQN-agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimizer\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "#create a global step coubter\n",
    "#train_step_counter = tf.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "#create agent\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    #train_step_counter=train_step_counter)\n",
    "    train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "selected agent collect_policy\n"
     ]
    }
   ],
   "source": [
    "magent = learningHelper(train_env=train_env, test_env=eval_env, agent=agent, global_step=global_step, collect_episodes = 10,\n",
    "eval_interval=5, verbose=0, batch_size=64, train_sequence_length=4, chkpdir='./rnn_chkp/')\n",
    "magent.restore_check_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Traning Loss 158700.34375\n",
      "Global steps 249: Traning Loss 118368.703125\n",
      "Global steps 250: Traning Loss 120660.703125\n",
      "Global steps 251: Traning Loss 83246.71875\n",
      "Global steps 252: Traning Loss 87649.125\n",
      "Global steps 253: Traning Loss 77563.2890625\n",
      "Global steps 254: Traning Loss 130366.2734375\n",
      "Global steps 255: Traning Loss 116801.796875\n",
      "Global steps 256: Traning Loss 160684.6875\n",
      "Global steps 257: Traning Loss 182066.15625\n",
      "Global steps 258: Traning Loss 206565.28125\n",
      "Global steps 259: Traning Loss 104983.953125\n",
      "Global steps 260: Traning Loss 150390.71875\n",
      "Global steps 261: Traning Loss 75692.34375\n",
      "Global steps 262: Traning Loss 138266.625\n",
      "Global steps 263: Traning Loss 89768.1171875\n",
      "Global steps 264: Traning Loss 220452.375\n",
      "Global steps 265: Traning Loss 121568.1015625\n",
      "Global steps 266: Traning Loss 98701.234375\n",
      "Global steps 267: Traning Loss 136979.46875\n",
      "Global steps 268: Traning Loss 112515.84375\n",
      "Global steps 269: Traning Loss 143790.484375\n",
      "Global steps 270: Traning Loss 105747.4375\n",
      "Global steps 271: Traning Loss 91720.875\n",
      "Global steps 272: Traning Loss 177253.40625\n",
      "Global steps 273: Traning Loss 111476.546875\n",
      "Global steps 274: Traning Loss 106386.96875\n",
      "Global steps 275: Traning Loss 115398.078125\n",
      "Global steps 276: Traning Loss 178235.625\n",
      "Global steps 277: Traning Loss 102177.84375\n",
      "Global steps 278: Traning Loss 169316.765625\n",
      "Global steps 279: Traning Loss 150304.09375\n",
      "Global steps 280: Traning Loss 141889.34375\n",
      "Global steps 281: Traning Loss 138488.578125\n",
      "Global steps 282: Traning Loss 130833.4296875\n",
      "Global steps 283: Traning Loss 112460.71875\n",
      "Global steps 284: Traning Loss 126389.25\n",
      "Global steps 285: Traning Loss 185025.015625\n",
      "Global steps 286: Traning Loss 188833.171875\n",
      "Global steps 287: Traning Loss 98489.421875\n",
      "Global steps 288: Traning Loss 83056.46875\n",
      "Global steps 289: Traning Loss 150438.0625\n",
      "Global steps 290: Traning Loss 102919.828125\n",
      "Global steps 291: Traning Loss 141268.46875\n",
      "Global steps 292: Traning Loss 118348.34375\n",
      "Global steps 293: Traning Loss 149733.0\n",
      "Global steps 294: Traning Loss 153801.078125\n",
      "Global steps 295: Traning Loss 174367.875\n",
      "Global steps 296: Traning Loss 101526.890625\n",
      "Global steps 297: Traning Loss 127869.53125\n",
      "Global steps 298: Traning Loss 121587.578125\n",
      "Global steps 299: Traning Loss 129546.390625\n",
      "Global steps 300: Traning Loss 85340.5390625\n",
      "Global steps 301: Traning Loss 95578.5\n",
      "Global steps 302: Traning Loss 102639.765625\n",
      "Global steps 303: Traning Loss 116646.7109375\n",
      "Global steps 304: Traning Loss 111719.453125\n",
      "Global steps 305: Traning Loss 107494.3125\n",
      "Global steps 306: Traning Loss 115827.2890625\n",
      "Global steps 307: Traning Loss 138960.921875\n",
      "Global steps 308: Traning Loss 122568.125\n",
      "Global steps 309: Traning Loss 113886.9609375\n",
      "Global steps 310: Traning Loss 126321.734375\n",
      "Global steps 311: Traning Loss 112112.921875\n",
      "Global steps 312: Traning Loss 154919.84375\n",
      "Global steps 313: Traning Loss 94841.1796875\n",
      "Global steps 314: Traning Loss 149209.46875\n",
      "Global steps 315: Traning Loss 104143.546875\n",
      "Global steps 316: Traning Loss 222111.03125\n",
      "Global steps 317: Traning Loss 137181.40625\n",
      "Global steps 318: Traning Loss 111724.28125\n",
      "Global steps 319: Traning Loss 113834.4765625\n",
      "Global steps 320: Traning Loss 112141.515625\n",
      "Global steps 321: Traning Loss 119263.703125\n",
      "Global steps 322: Traning Loss 93882.84375\n",
      "Global steps 323: Traning Loss 78503.484375\n",
      "Global steps 324: Traning Loss 79624.484375\n",
      "Global steps 325: Traning Loss 50918.16796875\n",
      "Global steps 326: Traning Loss 153389.53125\n",
      "Global steps 327: Traning Loss 96114.296875\n",
      "Global steps 328: Traning Loss 44715.1015625\n",
      "Global steps 329: Traning Loss 51663.8671875\n",
      "Global steps 330: Traning Loss 67735.0390625\n",
      "Global steps 331: Traning Loss 40721.00390625\n",
      "Global steps 332: Traning Loss 107436.859375\n",
      "Global steps 333: Traning Loss 58342.4296875\n",
      "Global steps 334: Traning Loss 113540.28125\n",
      "Global steps 335: Traning Loss 97325.6953125\n",
      "Global steps 336: Traning Loss 170215.34375\n",
      "Global steps 337: Traning Loss 112506.875\n",
      "Global steps 338: Traning Loss 113967.515625\n",
      "Global steps 339: Traning Loss 152930.46875\n",
      "Global steps 340: Traning Loss 171701.125\n",
      "Global steps 341: Traning Loss 172722.234375\n",
      "Global steps 342: Traning Loss 169209.9375\n",
      "Global steps 343: Traning Loss 176798.078125\n",
      "Global steps 344: Traning Loss 125343.625\n",
      "Global steps 345: Traning Loss 112124.234375\n",
      "Global steps 346: Traning Loss 183709.25\n",
      "Global steps 347: Traning Loss 122690.5625\n",
      "Global steps 348: Traning Loss 99692.671875\n",
      "Global steps 349: Traning Loss 171741.21875\n",
      "Global steps 350: Traning Loss 160267.3125\n",
      "Global steps 351: Traning Loss 110568.0546875\n",
      "Global steps 352: Traning Loss 94973.09375\n",
      "Global steps 353: Traning Loss 108420.2734375\n",
      "Global steps 354: Traning Loss 99064.4453125\n",
      "Global steps 355: Traning Loss 247055.5625\n",
      "Global steps 356: Traning Loss 134211.625\n",
      "Global steps 357: Traning Loss 57864.49609375\n",
      "Global steps 358: Traning Loss 112925.578125\n",
      "Global steps 359: Traning Loss 147700.84375\n",
      "Global steps 360: Traning Loss 93412.609375\n",
      "Global steps 361: Traning Loss 170498.6875\n",
      "Global steps 362: Traning Loss 106136.25\n",
      "Global steps 363: Traning Loss 161234.25\n",
      "Global steps 364: Traning Loss 178903.5\n",
      "Global steps 365: Traning Loss 120966.0\n",
      "Global steps 366: Traning Loss 120527.125\n",
      "Global steps 367: Traning Loss 121007.5390625\n",
      "Global steps 368: Traning Loss 125406.7109375\n",
      "Global steps 369: Traning Loss 136932.828125\n",
      "Global steps 370: Traning Loss 140837.125\n",
      "Global steps 371: Traning Loss 112820.0546875\n",
      "Global steps 372: Traning Loss 112549.0\n",
      "Global steps 373: Traning Loss 193303.75\n",
      "Global steps 374: Traning Loss 63727.15234375\n",
      "Global steps 375: Traning Loss 92350.421875\n",
      "Global steps 376: Traning Loss 120648.453125\n",
      "Global steps 377: Traning Loss 115486.71875\n",
      "Global steps 378: Traning Loss 88503.25\n",
      "Global steps 379: Traning Loss 134841.8125\n",
      "Global steps 380: Traning Loss 125776.578125\n",
      "Global steps 381: Traning Loss 132301.859375\n",
      "Global steps 382: Traning Loss 121382.7890625\n",
      "Global steps 383: Traning Loss 54363.03125\n",
      "Global steps 384: Traning Loss 126519.40625\n",
      "Global steps 385: Traning Loss 110741.9375\n",
      "Global steps 386: Traning Loss 83812.296875\n",
      "Global steps 387: Traning Loss 86819.0\n",
      "Global steps 388: Traning Loss 124354.1484375\n",
      "Global steps 389: Traning Loss 116650.5546875\n",
      "Global steps 390: Traning Loss 68179.25\n",
      "Global steps 391: Traning Loss 86262.828125\n",
      "Global steps 392: Traning Loss 153527.03125\n",
      "Global steps 393: Traning Loss 170150.375\n",
      "Global steps 394: Traning Loss 133995.0\n",
      "Global steps 395: Traning Loss 93945.8984375\n",
      "Global steps 396: Traning Loss 103859.265625\n",
      "Global steps 397: Traning Loss 176393.203125\n",
      "Global steps 398: Traning Loss 121604.90625\n",
      "Global steps 399: Traning Loss 73019.3671875\n",
      "Global steps 400: Traning Loss 96675.703125\n",
      "Global steps 401: Traning Loss 84304.28125\n",
      "Global steps 402: Traning Loss 137465.34375\n",
      "Global steps 403: Traning Loss 119351.9375\n",
      "Global steps 404: Traning Loss 103869.484375\n",
      "Global steps 405: Traning Loss 117472.9609375\n",
      "Global steps 406: Traning Loss 145108.25\n",
      "Global steps 407: Traning Loss 89201.765625\n",
      "Global steps 408: Traning Loss 117084.90625\n",
      "Global steps 409: Traning Loss 195623.25\n",
      "Global steps 410: Traning Loss 90438.59375\n",
      "Global steps 411: Traning Loss 57581.3046875\n",
      "Global steps 412: Traning Loss 110300.75\n",
      "Global steps 413: Traning Loss 104467.796875\n",
      "Global steps 414: Traning Loss 53446.578125\n",
      "Global steps 415: Traning Loss 34695.67578125\n",
      "Global steps 416: Traning Loss 30953.798828125\n",
      "Global steps 417: Traning Loss 172977.671875\n",
      "Global steps 418: Traning Loss 135061.125\n",
      "Global steps 419: Traning Loss 68976.828125\n",
      "Global steps 420: Traning Loss 153234.1875\n",
      "Global steps 421: Traning Loss 79909.609375\n",
      "Global steps 422: Traning Loss 85807.078125\n",
      "Global steps 423: Traning Loss 86387.4453125\n",
      "Global steps 424: Traning Loss 152557.75\n",
      "Global steps 425: Traning Loss 65209.02734375\n",
      "Global steps 426: Traning Loss 118607.078125\n",
      "Global steps 427: Traning Loss 110773.078125\n",
      "Global steps 428: Traning Loss 66847.9375\n",
      "Global steps 429: Traning Loss 90600.5625\n",
      "Global steps 430: Traning Loss 151858.125\n",
      "Global steps 431: Traning Loss 117146.703125\n",
      "Global steps 432: Traning Loss 117606.609375\n",
      "Global steps 433: Traning Loss 146278.28125\n",
      "Global steps 434: Traning Loss 79595.984375\n",
      "Global steps 435: Traning Loss 138458.53125\n",
      "Global steps 436: Traning Loss 66066.390625\n",
      "Global steps 437: Traning Loss 172279.671875\n",
      "Global steps 438: Traning Loss 182574.28125\n",
      "Global steps 439: Traning Loss 198473.984375\n",
      "Global steps 440: Traning Loss 128428.15625\n",
      "Global steps 441: Traning Loss 98638.640625\n",
      "Global steps 442: Traning Loss 70212.921875\n",
      "Global steps 443: Traning Loss 81981.0546875\n",
      "Global steps 444: Traning Loss 117281.9921875\n",
      "Global steps 445: Traning Loss 106843.53125\n",
      "Global steps 446: Traning Loss 102473.15625\n",
      "Global steps 447: Traning Loss 92983.09375\n",
      "Global steps 448: Traning Loss 21117.3984375\n",
      "Global steps 449: Traning Loss 127954.0625\n",
      "Global steps 450: Traning Loss 104560.0625\n",
      "Global steps 451: Traning Loss 81635.3359375\n",
      "Global steps 452: Traning Loss 112851.328125\n",
      "Global steps 453: Traning Loss 112990.9140625\n",
      "Global steps 454: Traning Loss 147155.5625\n",
      "Global steps 455: Traning Loss 140550.40625\n",
      "Global steps 456: Traning Loss 137550.421875\n",
      "Global steps 457: Traning Loss 94335.265625\n",
      "Global steps 458: Traning Loss 122756.125\n",
      "Global steps 459: Traning Loss 115509.84375\n",
      "Global steps 460: Traning Loss 115124.34375\n",
      "Global steps 461: Traning Loss 101850.46875\n",
      "Global steps 462: Traning Loss 75221.53125\n",
      "Global steps 463: Traning Loss 128844.546875\n",
      "Global steps 464: Traning Loss 79468.4765625\n",
      "Global steps 465: Traning Loss 59000.19140625\n",
      "Global steps 466: Traning Loss 66047.6640625\n",
      "Global steps 467: Traning Loss 91975.0078125\n",
      "Global steps 468: Traning Loss 53120.26171875\n",
      "Global steps 469: Traning Loss 42605.046875\n",
      "Global steps 470: Traning Loss 93644.453125\n",
      "Global steps 471: Traning Loss 75766.578125\n",
      "Global steps 472: Traning Loss 113985.6328125\n",
      "Global steps 473: Traning Loss 122216.6796875\n",
      "Global steps 474: Traning Loss 110366.0234375\n",
      "Global steps 475: Traning Loss 112767.265625\n",
      "Global steps 476: Traning Loss 119087.9453125\n",
      "Global steps 477: Traning Loss 77270.25\n",
      "Global steps 478: Traning Loss 91567.28125\n",
      "Global steps 479: Traning Loss 88564.0546875\n",
      "Global steps 480: Traning Loss 121231.375\n",
      "Global steps 481: Traning Loss 150571.0\n",
      "Global steps 482: Traning Loss 147111.5\n",
      "Global steps 483: Traning Loss 120847.8671875\n",
      "Global steps 484: Traning Loss 111351.828125\n",
      "Global steps 485: Traning Loss 90046.578125\n",
      "Global steps 486: Traning Loss 91200.453125\n",
      "Global steps 487: Traning Loss 59050.359375\n",
      "Global steps 488: Traning Loss 165299.0625\n",
      "Global steps 489: Traning Loss 129081.078125\n",
      "Global steps 490: Traning Loss 72549.4296875\n",
      "Global steps 491: Traning Loss 92705.546875\n",
      "Global steps 492: Traning Loss 112230.1328125\n",
      "Global steps 493: Traning Loss 167907.953125\n",
      "Global steps 494: Traning Loss 136021.109375\n",
      "Global steps 495: Traning Loss 77208.7734375\n",
      "Global steps 496: Traning Loss 67355.5078125\n",
      "Global steps 497: Traning Loss 79325.2421875\n",
      "Global steps 498: Traning Loss 97735.34375\n",
      "Global steps 499: Traning Loss 53274.515625\n",
      "Global steps 500: Traning Loss 120421.21875\n",
      "Global steps 501: Traning Loss 150577.671875\n",
      "Global steps 502: Traning Loss 147169.21875\n",
      "Global steps 503: Traning Loss 105626.875\n",
      "Global steps 504: Traning Loss 80977.5859375\n",
      "Global steps 505: Traning Loss 119921.0546875\n",
      "Global steps 506: Traning Loss 68090.984375\n",
      "Global steps 507: Traning Loss 102121.15625\n",
      "Global steps 508: Traning Loss 69738.5625\n",
      "Global steps 509: Traning Loss 91789.8515625\n",
      "Global steps 510: Traning Loss 89367.8125\n",
      "Global steps 511: Traning Loss 161200.484375\n",
      "Global steps 512: Traning Loss 136400.203125\n",
      "Global steps 513: Traning Loss 90366.9296875\n",
      "Global steps 514: Traning Loss 175612.765625\n",
      "Global steps 515: Traning Loss 98231.5625\n",
      "Global steps 516: Traning Loss 133872.984375\n",
      "Global steps 517: Traning Loss 171226.84375\n",
      "Global steps 518: Traning Loss 158997.625\n",
      "Global steps 519: Traning Loss 98819.203125\n",
      "Global steps 520: Traning Loss 77237.359375\n",
      "Global steps 521: Traning Loss 115829.1640625\n",
      "Global steps 522: Traning Loss 172876.25\n",
      "Global steps 523: Traning Loss 94319.515625\n",
      "Global steps 524: Traning Loss 134015.734375\n",
      "Global steps 525: Traning Loss 97586.25\n",
      "Global steps 526: Traning Loss 129440.75\n",
      "Global steps 527: Traning Loss 65098.484375\n",
      "Global steps 528: Traning Loss 117221.78125\n",
      "Global steps 529: Traning Loss 97523.796875\n",
      "Global steps 530: Traning Loss 113990.734375\n",
      "Global steps 531: Traning Loss 77173.296875\n",
      "Global steps 532: Traning Loss 127601.96875\n",
      "Global steps 533: Traning Loss 99573.984375\n",
      "Global steps 534: Traning Loss 63350.86328125\n",
      "Global steps 535: Traning Loss 145077.46875\n",
      "Global steps 536: Traning Loss 128405.4296875\n",
      "Global steps 537: Traning Loss 117429.140625\n",
      "Global steps 538: Traning Loss 154980.859375\n",
      "Global steps 539: Traning Loss 113032.03125\n",
      "Global steps 540: Traning Loss 129664.6640625\n",
      "Global steps 541: Traning Loss 168192.1875\n",
      "Global steps 542: Traning Loss 114348.1484375\n",
      "Global steps 543: Traning Loss 80475.453125\n",
      "Global steps 544: Traning Loss 101236.78125\n",
      "Global steps 545: Traning Loss 103540.1796875\n",
      "Global steps 546: Traning Loss 166962.625\n",
      "Global steps 547: Traning Loss 84394.53125\n",
      "Global steps 548: Traning Loss 147186.9375\n",
      "Global steps 549: Traning Loss 115768.4375\n",
      "Global steps 550: Traning Loss 117881.4453125\n",
      "Global steps 551: Traning Loss 187968.640625\n",
      "Global steps 552: Traning Loss 134317.875\n",
      "Global steps 553: Traning Loss 88949.109375\n",
      "Global steps 554: Traning Loss 133746.78125\n",
      "Global steps 555: Traning Loss 92583.09375\n",
      "Global steps 556: Traning Loss 123122.75\n",
      "Global steps 557: Traning Loss 73101.671875\n",
      "Global steps 558: Traning Loss 60690.50390625\n",
      "Global steps 559: Traning Loss 74353.8046875\n",
      "Global steps 560: Traning Loss 125902.203125\n",
      "Global steps 561: Traning Loss 99299.546875\n",
      "Global steps 562: Traning Loss 134045.03125\n",
      "Global steps 563: Traning Loss 117547.75\n",
      "Global steps 564: Traning Loss 116571.5\n",
      "Global steps 565: Traning Loss 182708.953125\n",
      "Global steps 566: Traning Loss 80058.2734375\n",
      "Global steps 567: Traning Loss 187083.296875\n",
      "Global steps 568: Traning Loss 192017.9375\n",
      "Global steps 569: Traning Loss 170973.84375\n",
      "Global steps 570: Traning Loss 129151.796875\n",
      "Global steps 571: Traning Loss 137334.09375\n",
      "Global steps 572: Traning Loss 78520.9453125\n",
      "Global steps 573: Traning Loss 67750.625\n",
      "Global steps 574: Traning Loss 79616.625\n",
      "Global steps 575: Traning Loss 83680.484375\n",
      "Global steps 576: Traning Loss 93233.015625\n",
      "Global steps 577: Traning Loss 73837.8203125\n",
      "Global steps 578: Traning Loss 158047.796875\n",
      "Global steps 579: Traning Loss 37826.04296875\n",
      "Global steps 580: Traning Loss 97136.859375\n",
      "Global steps 581: Traning Loss 103641.625\n",
      "Global steps 582: Traning Loss 56879.4609375\n",
      "Global steps 583: Traning Loss 115680.5546875\n",
      "Global steps 584: Traning Loss 64971.109375\n",
      "Global steps 585: Traning Loss 142371.96875\n",
      "Global steps 586: Traning Loss 152665.421875\n",
      "Global steps 587: Traning Loss 212852.921875\n",
      "Global steps 588: Traning Loss 241171.328125\n",
      "Global steps 589: Traning Loss 70548.4921875\n",
      "Global steps 590: Traning Loss 38712.703125\n",
      "Global steps 591: Traning Loss 90908.09375\n",
      "Global steps 592: Traning Loss 120739.671875\n",
      "Global steps 593: Traning Loss 168399.0625\n",
      "Global steps 594: Traning Loss 123156.625\n",
      "Global steps 595: Traning Loss 137293.921875\n",
      "Global steps 596: Traning Loss 173086.25\n",
      "Global steps 597: Traning Loss 166091.34375\n",
      "Global steps 598: Traning Loss 179120.125\n",
      "Global steps 599: Traning Loss 45487.01953125\n",
      "Global steps 600: Traning Loss 69453.4921875\n",
      "Global steps 601: Traning Loss 67159.3671875\n",
      "Global steps 602: Traning Loss 90360.8125\n",
      "Global steps 603: Traning Loss 180389.546875\n",
      "Global steps 604: Traning Loss 154277.15625\n",
      "Global steps 605: Traning Loss 130112.90625\n",
      "Global steps 606: Traning Loss 128559.9140625\n",
      "Global steps 607: Traning Loss 73351.34375\n",
      "Global steps 608: Traning Loss 110062.90625\n",
      "Global steps 609: Traning Loss 141114.453125\n",
      "Global steps 610: Traning Loss 129793.734375\n",
      "Global steps 611: Traning Loss 143244.5\n",
      "Global steps 612: Traning Loss 257629.390625\n",
      "Global steps 613: Traning Loss 84707.2109375\n",
      "Global steps 614: Traning Loss 221124.828125\n",
      "Global steps 615: Traning Loss 143450.0625\n",
      "Global steps 616: Traning Loss 105526.125\n",
      "Global steps 617: Traning Loss 118592.734375\n",
      "Global steps 618: Traning Loss 55308.8828125\n",
      "Global steps 619: Traning Loss 105782.390625\n",
      "Global steps 620: Traning Loss 144520.25\n",
      "Global steps 621: Traning Loss 110358.9453125\n",
      "Global steps 622: Traning Loss 168931.96875\n",
      "Global steps 623: Traning Loss 109952.0625\n",
      "Global steps 624: Traning Loss 98802.7734375\n",
      "Global steps 625: Traning Loss 141510.28125\n",
      "Global steps 626: Traning Loss 176172.015625\n",
      "Global steps 627: Traning Loss 123441.890625\n",
      "Global steps 628: Traning Loss 109281.1328125\n",
      "Global steps 629: Traning Loss 114947.1875\n",
      "Global steps 630: Traning Loss 119468.4921875\n",
      "Global steps 631: Traning Loss 78970.796875\n",
      "Global steps 632: Traning Loss 145850.890625\n",
      "Global steps 633: Traning Loss 140553.671875\n",
      "Global steps 634: Traning Loss 63374.95703125\n",
      "Global steps 635: Traning Loss 136913.90625\n",
      "Global steps 636: Traning Loss 140723.515625\n",
      "Global steps 637: Traning Loss 159434.46875\n",
      "Global steps 638: Traning Loss 111048.5703125\n",
      "Global steps 639: Traning Loss 60058.57421875\n",
      "Global steps 640: Traning Loss 112999.015625\n",
      "Global steps 641: Traning Loss 82782.421875\n",
      "Global steps 642: Traning Loss 87847.515625\n",
      "Global steps 643: Traning Loss 128398.65625\n",
      "Global steps 644: Traning Loss 133987.40625\n",
      "Global steps 645: Traning Loss 127224.875\n",
      "Global steps 646: Traning Loss 145691.78125\n",
      "Global steps 647: Traning Loss 71274.671875\n",
      "Global steps 648: Traning Loss 149140.6875\n",
      "Global steps 649: Traning Loss 92030.71875\n",
      "Global steps 650: Traning Loss 105205.859375\n",
      "Global steps 651: Traning Loss 152967.96875\n",
      "Global steps 652: Traning Loss 124987.140625\n",
      "Global steps 653: Traning Loss 234727.0625\n",
      "Global steps 654: Traning Loss 124937.140625\n",
      "Global steps 655: Traning Loss 137130.5625\n",
      "Global steps 656: Traning Loss 158263.1875\n",
      "Global steps 657: Traning Loss 122920.21875\n",
      "Global steps 658: Traning Loss 114962.109375\n",
      "Global steps 659: Traning Loss 119048.5859375\n",
      "Global steps 660: Traning Loss 104753.8359375\n",
      "Global steps 661: Traning Loss 106996.34375\n",
      "Global steps 662: Traning Loss 123068.71875\n",
      "Global steps 663: Traning Loss 102556.25\n",
      "Global steps 664: Traning Loss 204295.21875\n",
      "Global steps 665: Traning Loss 133502.625\n",
      "Global steps 666: Traning Loss 131535.0625\n",
      "Global steps 667: Traning Loss 97536.15625\n",
      "Global steps 668: Traning Loss 142997.203125\n",
      "Global steps 669: Traning Loss 116341.890625\n",
      "Global steps 670: Traning Loss 84019.96875\n",
      "Global steps 671: Traning Loss 81122.3125\n",
      "Global steps 672: Traning Loss 97260.671875\n",
      "Global steps 673: Traning Loss 123317.9296875\n",
      "Global steps 674: Traning Loss 148609.3125\n",
      "Global steps 675: Traning Loss 123127.4375\n",
      "Global steps 676: Traning Loss 96119.046875\n",
      "Global steps 677: Traning Loss 108581.65625\n",
      "Global steps 678: Traning Loss 243402.234375\n",
      "Global steps 679: Traning Loss 154398.203125\n",
      "Global steps 680: Traning Loss 109787.2890625\n",
      "Global steps 681: Traning Loss 93543.21875\n",
      "Global steps 682: Traning Loss 71823.8046875\n",
      "Global steps 683: Traning Loss 138533.78125\n",
      "Global steps 684: Traning Loss 214553.703125\n",
      "Global steps 685: Traning Loss 129879.765625\n",
      "Global steps 686: Traning Loss 183645.625\n",
      "Global steps 687: Traning Loss 163119.125\n",
      "Global steps 688: Traning Loss 125207.84375\n",
      "Global steps 689: Traning Loss 113134.890625\n",
      "Global steps 690: Traning Loss 99326.265625\n",
      "Global steps 691: Traning Loss 97369.4921875\n",
      "Global steps 692: Traning Loss 97862.296875\n",
      "Global steps 693: Traning Loss 114521.46875\n",
      "Global steps 694: Traning Loss 148014.546875\n",
      "Global steps 695: Traning Loss 77394.0625\n",
      "Global steps 696: Traning Loss 95812.4296875\n",
      "Global steps 697: Traning Loss 176461.671875\n",
      "Global steps 698: Traning Loss 129039.484375\n",
      "Global steps 699: Traning Loss 97374.1875\n",
      "Global steps 700: Traning Loss 132008.484375\n",
      "CPU times: user 21min 41s, sys: 51.1 s, total: 22min 32s\n",
      "Wall time: 19min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "magent.train_agent(600)\n",
    "#magent.train_agent_with_avg_ret_condition(50, 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eval episodes = 80: Average Return = 74.49990844726562\n",
      "eval episodes = 80: Average Return = 83.89998626708984\n",
      "eval episodes = 80: Average Return = 545.6998901367188\n",
      "eval episodes = 80: Average Return = -88.49996185302734\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TFDeque.mean at 0x7fcba0639cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TFDeque.mean at 0x7fcba0639cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "eval episodes = 80: Average Return = 261.39990234375\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba0636290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba0636290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "eval episodes = 80: Average Return = -61.699981689453125\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba06497a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba06497a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "eval episodes = 80: Average Return = 349.20001220703125\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba05eaef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba05eaef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "eval episodes = 80: Average Return = -49.59981918334961\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcbe8155dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcbe8155dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "eval episodes = 80: Average Return = 188.2997589111328\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba0575950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TFDeque.mean at 0x7fcba0575950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "eval episodes = 80: Average Return = 274.60015869140625\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    magent.evaluate_agent(80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "magent.store_check_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "magent.restore_check_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}